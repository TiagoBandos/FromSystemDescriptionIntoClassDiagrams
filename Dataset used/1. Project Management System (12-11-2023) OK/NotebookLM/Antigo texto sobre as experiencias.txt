We run two main scenarios:
\begin{enumerate}
    \item Directly prompting the system description
    \item Uploading a full Software Requirement Specification (SRS)
\end{enumerate}

For the first scenario we used the system description~ \ref{lst:system_description} previously used in the benchmark tests. In this scenario we used the prompt~ \ref{lst:notebookLM_Prompt_First_Scenario} 
\begin{lstlisting}[caption={notebookLMPromptFirstScenario}, label={lst:notebookLM_Prompt_First_Scenario}]
You are a software engineer who is creating a <system name> who has several years of experience in creating UML models from system description.
<System Description>
The output must be presented using PlantUML code.
Do not add references and Notes in the PlantUML code
\end{lstlisting}
In our preliminary experiments, we discovered that unless the prompt explicitly forbade the inclusion of references or explanatory notes in the PlantUML code, the generated code frequently contained syntax errors and failed to compile. Using prompt~ \ref{lst:notebookLM_Prompt_First_Scenario}, we asked Notebook LM to produce three UML views—­a class diagram, a sequence diagram, and a component diagram. While the tool generated valid class and sequence diagrams, its component-diagram output intermixed class, component, and sequence constructs, making compilation impossible. This situation likely reflects the difficulty of expressing component diagrams in PlantUML, even with guidance from the official documentation.

To probe whether larger, commercial models would be able to generate components diagrams, we repeated the experiment with ChatGPT-4o, DeepSeek, and Claude Sonnet 4. ChatGPT-4o produced syntax errors on the first attempt but delivered a  syntax correct diagram on the second. DeepSeek’s output compiled cleanly yet amounted to little more than a class diagram annotated with component stereotypes, offering limited architectural insight. Only Claude Sonnet 4 generated a coherent, syntactically correct component diagram on the first try, as shown in Figure \ref{fig:componentDiagramsGenerated}.
\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Chapters/Figures/Claude_ComponentDiagram_SystemDescription.png}
        \caption*{Claude}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Chapters/Figures/Deepseek_ComponentDiagram_SystemDescription.png}
        \caption*{DeepSeek}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Chapters/Figures/ChatGPT_ComponentDiagram_SystemDescription.png}
        \caption*{ChatGPT}
    \end{minipage}
    \caption{Component Diagrams generated by the LLM models.}
    \label{fig:componentDiagramsGenerated}
\end{figure}

For the second scenario we used a SRS model from the dataset created by Alessio Ferrario \cite{ferrari_2018_7118517} to see if the NotebookLM would be able to generate UML models directly from the SRS document. 
The SRS file selected is the file 2009-inventory 2.0 present in the dataset. We created a new prompt, but very similar to the previously used.

\begin{lstlisting}[caption={notebookLMPromptSecondScenario}, label={lst:notebookLM_Prompt_Second_Scenario}]
You are a software engineer who has several years of experience in creating UML models from requirements specification.
Create a <UML model>  for  the the functional requirements 3.3 Manage Categories; 3.5 Add item to inventory; 3.7 Suggest Item price and 3.10 Sell item in the SRS paper about the inventory system.
The output must be presented using PlantUML code. Do not add references and Notes in the PlantUML code
\end{lstlisting}

In the initial experiments, when we prompted NotebookLM to generate a class diagram based on the full Software Requirements Specification (SRS), the resulting output provided only a high-level representation of the system. While syntactically correct, the diagram lacked the depth and specificity expected from a model grounded in the detailed functional requirements outlined in the SRS. When prompted to generate a sequence diagram, the model focused on only four out of the eleven specified functional requirements, indicating partial coverage and limited contextual understanding. Furthermore, when asked to generate a component diagram, NotebookLM produced PlantUML code containing syntax errors, which prevented successful compilation without manual corrections.
Given the limited granularity of the class diagram and the incomplete representation in the sequence and component diagrams, we opted to narrow the scope of subsequent prompts. We selected a subset of four functional requirements—3.3 Manage Categories, 3.5 Add Item to Inventory, 3.7 Suggest Item Price, and 3.10 Sell Item—chosen for their relevance to the business logic and their representativeness of key system functionalities. This more focused approach aimed to improve the quality, coherence, and interpretability of the generated UML diagrams.